{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils import set_seed\n",
    "from model.model import GPT, GPTConfig\n",
    "from model.trainer import Trainer, TrainerConfig\n",
    "from model.utils import sample\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = []\n",
    "        for i in text:\n",
    "            for j in set(i):\n",
    "                chars.append(j)           \n",
    "        chars = sorted(list(set(chars))) + ['<pad>']\n",
    "        data_size, vocab_size = len(text), len(chars)\n",
    "        print('Data has %d SMILES \\n%d unique characters.' % (data_size, vocab_size))        \n",
    "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + 1][0]\n",
    "        dix = [self.stoi[s] for s in chunk] + [self.stoi['<pad>']] * (self.block_size - len(chunk))\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1584663 SMILES \n",
      " 27 unique characters.\n"
     ]
    }
   ],
   "source": [
    "block_size = 64\n",
    "text = [i.strip() for i in open('./data/train.txt', 'r').readlines()]\n",
    "train_dataset = CharDataset(text, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/28/2021 01:45:05 - INFO - model.model -   number of parameters: 4.103680e+05\n"
     ]
    }
   ],
   "source": [
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=1, n_head=1, n_embd=32)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 3: train loss 1.52972. lr 3.272672e-04: 100%|██████████| 4/4 [00:10<00:00,  2.53s/it]\n",
      "epoch 2 iter 3: train loss 1.36611. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.53s/it]\n",
      "epoch 3 iter 3: train loss 1.31296. lr 2.966253e-04: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]\n",
      "epoch 4 iter 3: train loss 1.17030. lr 5.998770e-04: 100%|██████████| 4/4 [00:10<00:00,  2.65s/it]\n",
      "epoch 5 iter 3: train loss 1.02065. lr 2.794607e-04: 100%|██████████| 4/4 [00:10<00:00,  2.63s/it]\n",
      "epoch 6 iter 3: train loss 0.97034. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]\n",
      "epoch 7 iter 3: train loss 0.94268. lr 3.443226e-04: 100%|██████████| 4/4 [00:10<00:00,  2.69s/it]\n",
      "epoch 8 iter 3: train loss 0.85762. lr 5.947046e-04: 100%|██████████| 4/4 [00:10<00:00,  2.71s/it]\n",
      "epoch 9 iter 3: train loss 0.75360. lr 2.321759e-04: 100%|██████████| 4/4 [00:10<00:00,  2.67s/it]\n",
      "epoch 10 iter 3: train loss 0.73634. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.69s/it]\n",
      "epoch 11 iter 3: train loss 0.72933. lr 3.908942e-04: 100%|██████████| 4/4 [00:10<00:00,  2.65s/it]\n",
      "epoch 12 iter 3: train loss 0.65099. lr 5.820474e-04: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]\n",
      "epoch 13 iter 3: train loss 0.62354. lr 1.866137e-04: 100%|██████████| 4/4 [00:10<00:00,  2.68s/it]\n",
      "epoch 14 iter 3: train loss 0.61551. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.63s/it]\n",
      "epoch 15 iter 3: train loss 0.59477. lr 4.351574e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 16 iter 3: train loss 0.55476. lr 5.622271e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 17 iter 3: train loss 0.50880. lr 1.439311e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 18 iter 3: train loss 0.52718. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 19 iter 3: train loss 0.53239. lr 4.759879e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 20 iter 3: train loss 0.48414. lr 5.357469e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 21 iter 3: train loss 0.47017. lr 1.052122e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 22 iter 3: train loss 0.47617. lr 7.978657e-05: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 23 iter 3: train loss 0.44605. lr 5.123489e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 24 iter 3: train loss 0.42591. lr 5.032795e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 25 iter 3: train loss 0.42254. lr 7.144044e-05: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]\n",
      "epoch 26 iter 3: train loss 0.42683. lr 1.149472e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 27 iter 3: train loss 0.42557. lr 5.433168e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 28 iter 3: train loss 0.40995. lr 4.656493e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 29 iter 3: train loss 0.40797. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 30 iter 3: train loss 0.39751. lr 1.548076e-04: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]\n",
      "epoch 31 iter 3: train loss 0.40563. lr 5.681051e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 32 iter 3: train loss 0.38480. lr 4.238121e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 33 iter 3: train loss 0.37694. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 34 iter 3: train loss 0.38009. lr 1.983555e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 35 iter 3: train loss 0.36237. lr 5.860843e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 36 iter 3: train loss 0.35889. lr 3.788304e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 37 iter 3: train loss 0.35526. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 38 iter 3: train loss 0.35363. lr 2.444849e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 39 iter 3: train loss 0.36282. lr 5.967978e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 40 iter 3: train loss 0.36213. lr 3.318467e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 41 iter 3: train loss 0.33939. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 42 iter 3: train loss 0.34546. lr 2.920242e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 43 iter 3: train loss 0.31504. lr 5.999735e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 44 iter 3: train loss 0.33594. lr 2.840541e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 45 iter 3: train loss 0.32018. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 46 iter 3: train loss 0.31987. lr 3.397661e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 47 iter 3: train loss 0.33232. lr 5.955307e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 48 iter 3: train loss 0.32417. lr 2.366665e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 49 iter 3: train loss 0.33350. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 50 iter 3: train loss 0.31618. lr 3.864981e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 51 iter 3: train loss 0.32054. lr 5.835822e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 52 iter 3: train loss 0.30526. lr 1.908874e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 53 iter 3: train loss 0.30116. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 54 iter 3: train loss 0.31373. lr 4.310332e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 55 iter 3: train loss 0.30941. lr 5.644316e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 56 iter 3: train loss 0.30453. lr 1.478795e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 57 iter 3: train loss 0.29340. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 58 iter 3: train loss 0.28388. lr 4.722404e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 59 iter 3: train loss 0.29127. lr 5.385652e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 60 iter 3: train loss 0.29749. lr 1.087350e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 61 iter 3: train loss 0.27592. lr 7.668741e-05: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 62 iter 3: train loss 0.29647. lr 5.090733e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 63 iter 3: train loss 0.29449. lr 5.066399e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 64 iter 3: train loss 0.28578. lr 7.444806e-05: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 65 iter 3: train loss 0.27817. lr 1.113470e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 66 iter 3: train loss 0.26579. lr 5.405962e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 67 iter 3: train loss 0.28529. lr 4.694665e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 68 iter 3: train loss 0.28313. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 69 iter 3: train loss 0.26072. lr 1.507978e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 70 iter 3: train loss 0.27277. lr 5.660087e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 71 iter 3: train loss 0.27397. lr 4.279891e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 72 iter 3: train loss 0.26660. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 73 iter 3: train loss 0.26006. lr 1.940379e-04: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "epoch 74 iter 3: train loss 0.25964. lr 5.846654e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 75 iter 3: train loss 0.27149. lr 3.832612e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 76 iter 3: train loss 0.25879. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]\n",
      "epoch 77 iter 3: train loss 0.25836. lr 2.399692e-04: 100%|██████████| 4/4 [00:10<00:00,  2.62s/it]\n",
      "epoch 78 iter 3: train loss 0.25384. lr 5.960923e-04: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]\n",
      "epoch 79 iter 3: train loss 0.26715. lr 3.364187e-04: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "epoch 80 iter 3: train loss 0.25150. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.59s/it]\n",
      "epoch 81 iter 3: train loss 0.25755. lr 2.874250e-04: 100%|██████████| 4/4 [00:11<00:00,  2.81s/it]\n",
      "epoch 82 iter 3: train loss 0.25542. lr 5.999994e-04: 100%|██████████| 4/4 [00:10<00:00,  2.62s/it]\n",
      "epoch 83 iter 3: train loss 0.24954. lr 2.886513e-04: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]\n",
      "epoch 84 iter 3: train loss 0.24217. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]\n",
      "epoch 85 iter 3: train loss 0.24371. lr 3.352003e-04: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]\n",
      "epoch 86 iter 3: train loss 0.24614. lr 5.962873e-04: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]\n",
      "epoch 87 iter 3: train loss 0.26177. lr 2.411720e-04: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]\n",
      "epoch 88 iter 3: train loss 0.26210. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]\n",
      "epoch 89 iter 3: train loss 0.24385. lr 3.820815e-04: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]\n",
      "epoch 90 iter 3: train loss 0.25205. lr 5.850503e-04: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]\n",
      "epoch 91 iter 3: train loss 0.23430. lr 1.951869e-04: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]\n",
      "epoch 92 iter 3: train loss 0.24777. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]\n",
      "epoch 93 iter 3: train loss 0.24488. lr 4.268782e-04: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]\n",
      "epoch 94 iter 3: train loss 0.25962. lr 5.665739e-04: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]\n",
      "epoch 95 iter 3: train loss 0.25862. lr 1.518637e-04: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]\n",
      "epoch 96 iter 3: train loss 0.23698. lr 6.000000e-05: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]\n",
      "epoch 97 iter 3: train loss 0.23096. lr 4.684524e-04: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]\n",
      "epoch 98 iter 3: train loss 0.24072. lr 5.413273e-04: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]\n",
      "epoch 99 iter 3: train loss 0.23610. lr 1.123027e-04: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]\n",
      "epoch 100 iter 3: train loss 0.24177. lr 7.364081e-05: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "tconf = TrainerConfig(max_epochs=100, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1=CCN=C(c2ccccc2)C1<pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:rdkit=\"http://www.rdkit.org/xml\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.1\" baseProfile=\"full\" xml:space=\"preserve\" width=\"300px\" height=\"300px\" viewBox=\"0 0 300 300\">\n",
       "<!-- END OF HEADER -->\n",
       "<rect style=\"opacity:1.0;fill:#FFFFFF;stroke:none\" width=\"300\" height=\"300\" x=\"0\" y=\"0\"> </rect>\n",
       "<path class=\"bond-0 atom-0 atom-1\" d=\"M 40.9091,201.1 L 13.6364,153.862\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-0 atom-0 atom-1\" d=\"M 46.2657,188.559 L 27.1748,155.493\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-11 atom-11 atom-0\" d=\"M 95.4545,201.1 L 40.9091,201.1\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-1 atom-1 atom-2\" d=\"M 13.6364,153.862 L 40.9091,106.624\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-2 atom-2 atom-3\" d=\"M 40.9091,106.624 L 64.5709,106.624\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-2 atom-2 atom-3\" d=\"M 64.5709,106.624 L 88.2327,106.624\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-3 atom-3 atom-4\" d=\"M 100.701,115.711 L 111.714,134.787\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-3 atom-3 atom-4\" d=\"M 111.714,134.787 L 122.727,153.862\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-3 atom-3 atom-4\" d=\"M 94.5575,126.888 L 102.267,140.241\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-3 atom-3 atom-4\" d=\"M 102.267,140.241 L 109.976,153.594\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-4 atom-4 atom-5\" d=\"M 122.727,153.862 L 177.273,153.862\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-10 atom-4 atom-11\" d=\"M 122.727,153.862 L 95.4545,201.1\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-5 atom-5 atom-6\" d=\"M 177.273,153.862 L 204.545,201.1\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-5 atom-5 atom-6\" d=\"M 190.811,155.493 L 209.902,188.559\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-12 atom-10 atom-5\" d=\"M 204.545,106.624 L 177.273,153.862\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-6 atom-6 atom-7\" d=\"M 204.545,201.1 L 259.091,201.1\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-7 atom-7 atom-8\" d=\"M 259.091,201.1 L 286.364,153.862\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-7 atom-7 atom-8\" d=\"M 253.734,188.559 L 272.825,155.493\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-8 atom-8 atom-9\" d=\"M 286.364,153.862 L 259.091,106.624\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-9 atom-9 atom-10\" d=\"M 259.091,106.624 L 204.545,106.624\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-9 atom-9 atom-10\" d=\"M 250.909,117.533 L 212.727,117.533\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"atom-3\" d=\"M 92.04 98.9004 L 97.1018 107.082 Q 97.6036 107.89, 98.4109 109.351 Q 99.2182 110.813, 99.2618 110.9 L 99.2618 98.9004 L 101.313 98.9004 L 101.313 114.348 L 99.1964 114.348 L 93.7636 105.402 Q 93.1309 104.355, 92.4545 103.155 Q 91.8 101.955, 91.6036 101.584 L 91.6036 114.348 L 89.5964 114.348 L 89.5964 98.9004 L 92.04 98.9004 \" fill=\"#0000FF\"/>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"C\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 25, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)\n",
    "\n",
    "smiles = re.sub(\"<pad>\",\"\",completion)\n",
    "m = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "def moltosvg(mol, molSize = (300,300), kekulize = True):\n",
    "    mc = Chem.Mol(mol.ToBinary())\n",
    "    if kekulize:\n",
    "        try:\n",
    "            Chem.Kekulize(mc)\n",
    "        except:\n",
    "            mc = Chem.Mol(mol.ToBinary())\n",
    "    if not mc.GetNumConformers():\n",
    "        rdDepictor.Compute2DCoords(mc)\n",
    "    drawer = rdMolDraw2D.MolDraw2DSVG(molSize[0],molSize[1])\n",
    "    drawer.DrawMolecule(mc)\n",
    "    drawer.FinishDrawing()\n",
    "    svg = drawer.GetDrawingText()\n",
    "    return svg.replace('svg:','')\n",
    "\n",
    "SVG(moltosvg(m))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
